\documentclass[12pt]{article}
% \documentstyle{iascars2017}

% \usepackage{iascars2017}

\pagestyle{myheadings} 
\pagenumbering{arabic}
\topmargin 0pt \headheight 23pt \headsep 24.66pt
%\topmargin 11pt \headheight 12pt \headsep 13.66pt
\parindent = 3mm 


\begin{document}


\begin{flushleft}


{\LARGE\bf Random Search Global Optimization using Random Forests}


\vspace{1.0cm}

B. L. Robertson$^1$, C. J. Price$^1$ and M. Reale$^1$

\begin{description}

\item $^1 \;$ School of Mathematics and Statistics, University of Canterbury,
Christchurch, NZ

\end{description}

\end{flushleft}

%  ***** ADD ENOUGH VERTICAL SPACE HERE TO ENSURE THAT THE *****
%  ***** ABSTRACT (OR MAIN TEXT) STARTS 5 CM BELOW THE TOP *****

\vspace{0.75cm}

\noindent {\bf Abstract}. The purpose of a global optimization algorithm is to efficiently find an objective function's global minimum. In this talk we consider bound constrained global optimization, where the search is performed in a box, denoted $\Omega$. The global optimization problem is deceptively simple and it is usually difficult to find the global minimum. One of the difficulties is that there is often no way to verify that a local minimum is indeed the global minimum. If the objective function is convex, the local minimum is also the global minimum. However, many optimization problems are not convex. Of particular interest in this talk are objective functions that lack any special properties such as continuity, smoothness, or a Lipschitz constant.

A random search algorithm for bound constrained global optimization is presented. This algorithm alternates between partition and sampling phases. At each iteration, points sampled from $\Omega$ are classified low or high based on their objective function values. These classified points define training data that is used to partition $\Omega$ into low and high regions using a random forest. The objective function is then evaluated at a number of points drawn from the low region and from $\Omega$ itself. Drawing points from the low region focuses the search in areas where the objective function is known to be low. Sampling $\Omega$ reduces the risk of missing the global minimum and is necessary to establish convergence. The new points are then added to the existing training data and the method repeats.

A preliminary simulation study showed that alternating between random forest partition and sampling phases was an effective strategy for solving a variety of global optimization test problems. The authors are currently refining the method and extending the set of test problems.

\vskip 2mm

\noindent {\bf Keywords}.
Bound constrained optimization, classification and regression trees (CART), stochastic optimization


\end{document}





