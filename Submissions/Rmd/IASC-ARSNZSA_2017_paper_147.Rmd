<span>**Abstract:**</span> It is beneficial for modeling data of
interest to exploit secondary information. The secondary information is
called auxiliary variables, which may not be observed in testing data
because they are not of primary interest. In this paper, we incorporate
the auxiliary variables into a framework of supervised learning.
Furthermore, we consider a covariate shift situation that allows a
density function of covariates to change between testing and training
data. It is known that the Maximum Log-likelihood Estimate (MLE) is not
a good estimator under model misspecification and the covariate shift.
This problem can be resolved by the Maximum Weighted Log-likelihood
Estimate (MWLE).

When we have multiple candidate models, it needs to select the best
candidate model where its optimality is measured by the expected
Kullback-Leibler (KL) divergence. The Akaike information criterion (AIC)
is a well known criterion based on the KL divergence and using the MLE.
Therefore, its validity is not guaranteed when the MWLE is used under
the covariate shift. An information criterion under the covariate shift
was proposed in Shimodaira (2000, JSPI) but this criterion does not take
use of the auxiliary variables into account. Hence, we resolve this
problem by deriving a new criterion. In addition, simulations are
conducted to examine the improvement.

<span>**Keywords:**</span> Auxiliary variables; Covariate shift;
Information criterion; Kullback-Leibler divergence; Misspecification;
Predictions.
